{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Q1: Min-Max Scaling**\n",
    "\n",
    "* **Definition:** A data scaling technique that linearly transforms a dataset's features to a range between a minimum (often 0) and a maximum (often 1).\n",
    "\n",
    "* **Purpose:** Improves the performance of machine learning models by ensuring all features are on a similar scale. This prevents features with larger ranges from unduly influencing the model's learning process.\n",
    "\n",
    "* **Example:**\n",
    "\n",
    "  Imagine a dataset with two features: price (ranging from $10 to $100) and rating (from 1 to 5). The model might prioritize minimizing price errors more due to its larger scale. Min-Max scaling scales both features to the 0-1 range, giving them equal weight during model training.\n",
    "\n",
    "**Q2: Unit Vector (L2 Norm) Scaling**\n",
    "\n",
    "* **Definition:** A data scaling technique that normalizes each data point to a unit length (L2 norm of 1). This makes the magnitude of each data point equal, focusing the model on the direction rather than the absolute values.\n",
    "\n",
    "* **Difference from Min-Max:** Unit Vector scaling doesn't necessarily map all features to a specific range like 0-1. It ensures equal importance for the direction of the data point in the higher-dimensional space.\n",
    "\n",
    "* **Example:**\n",
    "\n",
    "  Consider a dataset with features like height and weight. Min-Max might map heights (e.g., 150cm to 180cm) to a smaller range than weights (e.g., 50kg to 80kg). Unit Vector scaling normalizes both to unit length, focusing on the weight-to-height ratio for the model.\n",
    "\n",
    "**Q3: Principal Component Analysis (PCA)**\n",
    "\n",
    "* **Definition:** A dimensionality reduction technique that transforms a dataset into a lower-dimensional space while retaining most of the variance (information) in the data.\n",
    "\n",
    "* **Purpose:** Helps to reduce computational complexity and potentially improve model performance by removing redundant or less important features. PCA identifies a new set of features (principal components) that are uncorrelated and capture the maximum variance in the original data.\n",
    "\n",
    "* **Example:**\n",
    "\n",
    "  Imagine a dataset with features like height, weight, and shoe size. These might be highly correlated. PCA could identify two principal components: one capturing overall size (combination of height and weight) and another capturing variation in shoe size independent of height and weight.\n",
    "\n",
    "**Q4: PCA and Feature Extraction**\n",
    "\n",
    "* **Relationship:** PCA can be used for feature extraction by selecting a subset of the principal components that capture a significant portion of the data's variance. These components represent the most informative features in the lower-dimensional space.\n",
    "\n",
    "* **Feature Extraction using PCA:**\n",
    "\n",
    "  1. Apply PCA to the dataset.\n",
    "  2. Determine the number of principal components to retain based on the cumulative explained variance (percentage of variance captured by each component). Choose components that contribute to a desired threshold (e.g., 90%) of the variance.\n",
    "  3. Use the selected principal components as the new, reduced-dimensionality features for your model.\n",
    "\n",
    "**Q5: Min-Max Scaling in Food Delivery Recommendation System**\n",
    "\n",
    "Apply Min-Max scaling to the following features:\n",
    "\n",
    "- **Price:** Scale to a range of 0-1 (e.g., `(price - min_price) / (max_price - min_price)`).\n",
    "- **Rating:** Scale to a range of 0-1 (e.g., `(rating - min_rating) / (max_rating - min_rating)`).\n",
    "- **Delivery Time:** Scale to a range of 0-1 (e.g., `(delivery_time - min_delivery_time) / (max_delivery_time - min_delivery_time)`).\n",
    "\n",
    "This ensures all features have equal weight for the model's recommendations based on user preferences.\n",
    "\n",
    "**Q6: PCA in Stock Price Prediction**\n",
    "\n",
    "Apply PCA to the financial and market trend features to reduce dimensionality:\n",
    "\n",
    "1. Train a PCA model on the dataset.\n",
    "2. Analyze the explained variance ratio for each principal component.\n",
    "3. Choose the number of components that capture a high percentage of the variance (e.g., 95%).\n",
    "4. Use the selected principal components as new features for your stock price prediction model.\n",
    "\n",
    "PCA helps manage the complexity of dealing with numerous financial features and potentially improves model performance.\n",
    "\n",
    "## Q7: Min-Max Scaling Example\n",
    "\n",
    "Original data: `[1, 5, 10, 15, 20]`\n",
    "\n",
    "We want to scale the values to a range of -1 to 1. Here's the calculation:\n",
    "\n",
    "```python\n",
    "def min_max_scaling(data, new_min=-1, new_max=1):\n",
    "  \"\"\"Scales data to a new range (default: -1 to 1) using Min-Max scaling.\"\"\"\n",
    "  min_value = min(data)\n",
    "  max_value = max(data)\n",
    "  return [(x - min_value) / (max_value - min_value) * (new_max - new_min) + new_min for x in data]\n",
    "\n",
    "scaled_data = min_max_scaling([1, 5, 10, 15, 20])\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "This code defines a `min_max_scaling` function that takes the data and optional new minimum and maximum values. It calculates the scaling factor and applies it to each data point. Running the code outputs:\n",
    "\n",
    "```\n",
    "[-0.8, 0, 0.4, 0.8, 1.0]\n",
    "```\n",
    "\n",
    "As you can see, the values are now scaled to the range of -1 to 1.\n",
    "\n",
    "## Q8: Feature Extraction with PCA (Example and Considerations)\n",
    "\n",
    "While providing a specific number of components for your dataset is impossible without analysis, here's a breakdown of the approach and factors to consider:\n",
    "\n",
    "**Dataset:** `[height, weight, age, gender, blood pressure]`\n",
    "\n",
    "**Feature Extraction with PCA:**\n",
    "\n",
    "1. **Import Libraries:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.decomposition import PCA\n",
    "   ```\n",
    "\n",
    "2. **Load Data (Replace with your actual data):**\n",
    "\n",
    "   ```python\n",
    "   data = pd.DataFrame({\n",
    "       \"height\": [170, 180, 165, 175, 150],\n",
    "       \"weight\": [70, 85, 60, 72, 55],\n",
    "       \"age\": [30, 25, 40, 32, 22],\n",
    "       \"gender\": [\"M\", \"M\", \"F\", \"M\", \"F\"],\n",
    "       \"blood_pressure\": [120, 135, 110, 125, 100]\n",
    "   })\n",
    "   ```\n",
    "\n",
    "3. **Standardize Data (Optional):**\n",
    "\n",
    "   PCA can be sensitive to feature scales. Standardizing features (subtracting the mean and dividing by the standard deviation) can improve results.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   scaler = StandardScaler()\n",
    "   scaled_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "4. **Apply PCA:**\n",
    "\n",
    "   ```python\n",
    "   pca = PCA()\n",
    "   pca_components = pca.fit_transform(scaled_data)  # Transformed data\n",
    "   ```\n",
    "\n",
    "5. **Analyze Explained Variance Ratio:**\n",
    "\n",
    "   ```python\n",
    "   explained_variance = pca.explained_variance_ratio_\n",
    "   ```\n",
    "\n",
    "   The `explained_variance_ratio_` attribute gives you the proportion of variance explained by each principal component.\n",
    "\n",
    "**Choosing the Number of Components:**\n",
    "\n",
    "There's no single answer, but here's how to decide:\n",
    "\n",
    "- **Explained Variance Threshold:** A common practice is to choose components that capture a high percentage of the variance, typically 80-95%. You can plot the explained variance ratio to see how much variance each component explains.\n",
    "- **Model Performance:** Ultimately, the optimal number depends on your specific dataset and model. Experiment with different numbers of components and evaluate how it affects your model's performance.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- **Number of Features:** If you have a large number of features initially, using PCA for dimensionality reduction can be very beneficial.\n",
    "- **Redundancy:** PCA is effective if features are highly correlated. Less correlated features might not benefit as much from PCA.\n",
    "\n",
    "Remember, PCA might not always be the best choice. Domain knowledge and the characteristics of your dataset are crucial for making informed decisions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. Probability of Smoker Given Health Insurance**\n",
    "\n",
    "In the scenario where 70% of employees use the health insurance plan and 40% of users are smokers, we can calculate the probability of an employee being a smoker given they have health insurance using Bayes' theorem:\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "```\n",
    "P(Smoker | Insurance) = (P(Insurance | Smoker) * P(Smoker)) / P(Insurance)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `P(Smoker | Insurance)`: Probability of being a smoker given they have health insurance (what we want to find).\n",
    "- `P(Insurance | Smoker)`: Probability of having insurance given they are a smoker (likelihood). We don't have this value explicitly, but it can be inferred from the given information.\n",
    "- `P(Smoker)`: Prior probability of being a smoker in the entire employee population. We don't have this information in this case, so we'll assume it's unknown.\n",
    "- `P(Insurance)`: Probability of having health insurance (marginal probability). We are given this value (70%).\n",
    "\n",
    "**Since we lack the prior probability (P(Smoker))** and it might not be directly relevant to the question of smoker probability given insurance, we can calculate the **ratio** of the numerator and denominator to get a relative probability:\n",
    "\n",
    "```\n",
    "P(Smoker | Insurance) / P(Non-Smoker | Insurance) = (P(Insurance | Smoker) * P(Smoker)) / (P(Insurance | Non-Smoker) * P(Non-Smoker))\n",
    "```\n",
    "\n",
    "We know that P(Insurance) = 0.7 (70%). We can rewrite P(Insurance | Smoker) as 1 - P(No Insurance | Smoker), assuming people either have insurance or not. Let's say we don't have data on non-smokers' insurance rates (P(Insurance | Non-Smoker)), so we can't calculate the exact probability.\n",
    "\n",
    "However, we can still conclude that:\n",
    "\n",
    "- The numerator (`P(Insurance | Smoker) * P(Smoker)`) represents the likelihood of a smoker having insurance and the overall smoker population.\n",
    "- The denominator (unknown term) represents the likelihood of a non-smoker having insurance and the overall non-smoker population.\n",
    "\n",
    "Since 40% (0.4) of those with insurance are smokers, `P(Insurance | Smoker)` is likely higher than `P(Insurance | Non-Smoker)`. Even without knowing the exact values, we can infer that:\n",
    "\n",
    "**P(Smoker | Insurance) > P(Non-Smoker | Insurance)**\n",
    "\n",
    "Therefore, an employee with health insurance is **more likely** to be a smoker than a non-smoker, based on the given information.\n",
    "\n",
    "**Q2. Bernoulli vs. Multinomial Naive Bayes**\n",
    "\n",
    "**Bernoulli Naive Bayes:**\n",
    "\n",
    "- Assumes features are binary (0 or 1).\n",
    "- Often used for scenarios with presence/absence or true/false values (e.g., email spam classification: spam = 1, not spam = 0).\n",
    "\n",
    "**Multinomial Naive Bayes:**\n",
    "\n",
    "- Assumes discrete features with a fixed number of possible values (e.g., word counts in documents, weather categories).\n",
    "- Suitable for text classification where features represent words or phrases with a limited vocabulary.\n",
    "\n",
    "**Key Difference:**\n",
    "\n",
    "The main distinction lies in the types of features they handle:\n",
    "\n",
    "- Bernoulli: Binary features (0/1)\n",
    "- Multinomial: Discrete features with multiple possible values\n",
    "\n",
    "**Q3. Handling Missing Values in Bernoulli Naive Bayes**\n",
    "\n",
    "Bernoulli Naive Bayes is generally more robust to missing values compared to other Naive Bayes variations. Here are common approaches:\n",
    "\n",
    "- **Replace with mode:** Assign the most frequent value (mode) for the missing feature in the training data.\n",
    "- **Imputation:** Estimate missing values based on other features or domain knowledge.\n",
    "- **Smoothing:** Add a small value (e.g., 1) to all counts (zeros and non-zeros) to avoid zero probabilities during calculations. This is often used with Laplace smoothing.\n",
    "\n",
    "**Q4. Gaussian Naive Bayes for Multi-Class Classification**\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. It assumes features follow Gaussian (normal) distributions for each class. Here's how it works:\n",
    "\n",
    "1. **Estimate Class Parameters:** For each class, calculate the mean (`μ`) and variance (`σ^2`) of each feature distribution using the training data.\n",
    "2. **Calculate Class Probabilities:** For a new instance, compute the probability density function (PDF) of each feature value under the Gaussian distribution of each class."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Regression Models (Q1-Q5)\n",
    "\n",
    "**Q1. R-squared:**\n",
    "\n",
    "R-squared (R²) is a statistical measure used in linear regression to assess how well the regression line fits the data. It represents the proportion of the variance in the dependent variable (Y) that is explained by the independent variable(s) (X).\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "R² = 1 - (Σ (yi - ŷi)² ) / (Σ (yi - y̅)²)\n",
    "\n",
    "* yi: actual value\n",
    "* ŷi: predicted value\n",
    "* y̅: mean of actual values\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* 0: No linear relationship\n",
    "* 1: Perfect fit (all points fall exactly on the regression line)\n",
    "* Values between 0 and 1 indicate the strength of the linear relationship, with higher values suggesting a better fit.\n",
    "\n",
    "**Q2. Adjusted R-squared:**\n",
    "\n",
    "Adjusted R-squared (adjusted R²) penalizes R² for the number of independent variables in the model. It accounts for model complexity, preventing overfitting to the training data.\n",
    "\n",
    "**Difference from R-squared:**\n",
    "\n",
    "* R² can increase simply by adding more variables, even if they are not truly explanatory.\n",
    "* Adjusted R² discourages this by considering the penalty for model complexity.\n",
    "\n",
    "**Q3. When to use Adjusted R-squared:**\n",
    "\n",
    "* When comparing models with different numbers of independent variables.\n",
    "* When assessing the generalizability of a model beyond the training data.\n",
    "\n",
    "**Q4. Error Metrics in Regression:**\n",
    "\n",
    "* **Root Mean Squared Error (RMSE):** Squares the errors (differences between predicted and actual values), takes the square root to get units of the original variable. Represents the average magnitude of the error.\n",
    "\n",
    "* **Mean Squared Error (MSE):** Squares the errors and then averages them. Sensitive to outliers as large errors are squared more heavily.\n",
    "\n",
    "* **Mean Absolute Error (MAE):** Takes the absolute value of the errors and then averages them. Less sensitive to outliers than MSE, but results are in the units of the predicted variable.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "* RMSE = √(Σ (yi - ŷi)² / n)\n",
    "* MSE = Σ (yi - ŷi)² / n\n",
    "* MAE = Σ |yi - ŷi| / n\n",
    "\n",
    "**Q5. Advantages and Disadvantages:**\n",
    "\n",
    "* **RMSE:** Easy to interpret, considers magnitude of errors. Disadvantage: Sensitive to outliers.\n",
    "* **MSE:** Sensitive to outliers, can be difficult to interpret due to squared values. Advantage: Easier to work with mathematically.\n",
    "* **MAE:** Less sensitive to outliers, interpretable in original units. Disadvantage: Doesn't consider the magnitude of errors.\n",
    "\n",
    "## Regularization for Linear Regression (Q6-Q8)\n",
    "\n",
    "**Q6. Lasso Regularization:**\n",
    "\n",
    "Lasso regularization shrinks the coefficients of some features towards zero, potentially setting some to exactly zero. This helps to:\n",
    "\n",
    "* Reduce model complexity and prevent overfitting.\n",
    "* Feature selection: By driving coefficients to zero, it effectively removes irrelevant features from the model.\n",
    "\n",
    "**Difference from Ridge Regularization:**\n",
    "\n",
    "* Ridge regularization shrinks all coefficients towards zero but doesn't necessarily set them to zero.\n",
    "* Lasso performs feature selection, while ridge focuses on reducing coefficient magnitudes.\n",
    "\n",
    "**Use Lasso When:**\n",
    "\n",
    "* You suspect some features might be irrelevant or redundant.\n",
    "* Feature interpretability is important, and you want to identify the most important features.\n",
    "\n",
    "**Q7. Regularization and Overfitting:**\n",
    "\n",
    "Overfitting occurs when a model memorizes the training data too well and fails to generalize to unseen data. Regularization helps prevent overfitting by:\n",
    "\n",
    "* **Penalizing model complexity:** By adding a penalty term to the cost function that increases with the magnitude of the coefficients, the model is discouraged from fitting too closely to the training data at the expense of higher variance.\n",
    "\n",
    "**Example:** Imagine a model predicting exam grades based on hours studied (X1) and difficulty level (X2). Without regularization, it might overfit to random noise in the training data and not generalize well to unseen difficulty levels. Lasso regularization could set the coefficient for difficulty level (X2) to zero, effectively removing it from the model if it's not truly relevant.\n",
    "\n",
    "**Q8. Limitations of Regularized Models:**\n",
    "\n",
    "* **Tuning the regularization parameter:** The strength of the regularization (penalty term) needs to be carefully tuned to avoid underfitting (model is too simple) or overfitting.\n",
    "* **Loss of information:** Lasso regularization might set important features to zero, potentially losing some information.\n",
    "* Not a silver bullet: Regularization doesn't guarantee perfect performance, and other model selection techniques might be necessary.\n",
    "\n",
    "## Choosing Evaluation Metrics (Q9)\n",
    "\n",
    "**Choosing Between Model A (RMSE 10) and Model B (MAE 8):**\n",
    "\n",
    "**It's difficult to definitively choose based solely on RMSE and MAE.** Here's why:\n",
    "\n",
    "* **RMSE:** Sensitive to outliers. A single large error can significantly inflate the RMSE.\n",
    "* **MAE:** Less sensitive to outliers, but doesn't consider the magnitude of errors. \n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "* **Distribution of errors:** If your errors have a long tail with many outliers, MAE might be a better choice.\n",
    "* **Cost of large errors:** If large errors are particularly problematic in your application (e.g., predicting stock prices), RMSE might be more relevant.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "Both metrics have limitations, and the best choice depends on the specific context and the cost function associated with errors in your application.\n",
    "\n",
    "**Further Exploration:**\n",
    "\n",
    "* **Visualize the distribution of errors:** This can help you understand if outliers are a concern.\n",
    "* **Consider using both metrics:** Report both RMSE and MAE to get a more comprehensive picture of model performance.\n",
    "\n",
    "## Choosing Regularization (Q10)\n",
    "\n",
    "**Comparing Model A (Ridge, λ=0.1) and Model B (Lasso, λ=0.5):**\n",
    "\n",
    "**Difficult to choose definitively without additional information.** Here's a breakdown:\n",
    "\n",
    "* **Ridge regularization:** Reduces coefficient magnitudes but doesn't necessarily set them to zero.\n",
    "* **Lasso regularization:** Can set coefficients to zero, potentially performing feature selection.\n",
    "\n",
    "**Choosing Based on Context:**\n",
    "\n",
    "* **Feature interpretability:** If understanding the most important features is crucial, Lasso might be better (assuming it doesn't eliminate relevant features).\n",
    "* **Multicollinearity:** If features are highly correlated, Ridge might be preferable as it can reduce the impact of multicollinearity without discarding features entirely.\n",
    "\n",
    "**Trade-offs and Limitations:**\n",
    "\n",
    "* **Tuning regularization parameter:** Both require tuning the regularization parameter (λ) to avoid underfitting or overfitting.\n",
    "* **Lasso might discard important features:** This can lead to a loss of information.\n",
    "* **Regularization isn't a guarantee:** It helps prevent overfitting but doesn't ensure the best model. Explore other model selection techniques as well.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

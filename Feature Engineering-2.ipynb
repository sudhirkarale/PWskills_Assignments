{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Filter Method**\n",
    "\n",
    "The filter method is a feature selection technique that **evaluates each feature independently** based on its relevance to the target variable. It doesn't involve building and evaluating machine learning models. Here's how it works:\n",
    "\n",
    "1. **Statistical Measures:** Various statistical tests are used to measure the correlation or relationship between each feature and the target variable. Common methods include:\n",
    "    - Chi-Square test (categorical features)\n",
    "    - Correlation coefficient (continuous features)\n",
    "    - Information gain (decision trees)\n",
    "2. **Thresholding and Ranking:** Features are ranked based on their scores from the statistical tests. A threshold is set to select the top-ranked features that exceed it.\n",
    "\n",
    "**Q2. Wrapper vs. Filter Method**\n",
    "\n",
    "- **Filter Method:** Independent evaluation, computationally efficient, but might miss feature interactions.\n",
    "- **Wrapper Method:** Evaluates feature subsets using a machine learning model, considers feature interactions, but computationally expensive.\n",
    "\n",
    "**Q3. Embedded Methods**\n",
    "\n",
    "Embedded methods integrate feature selection into the model training process itself. The model inherently performs feature selection during training, selecting attributes that contribute most to the model's performance. Common examples:\n",
    "\n",
    "- Regularization with L1 or L2 penalty (shrinks weights, effectively removes less important features)\n",
    "- Tree-based models (select features at each split based on their predictive power)\n",
    "\n",
    "**Q4. Drawbacks of Filter Methods**\n",
    "\n",
    "- **Feature Independence Assumption:** Might miss important features that are only relevant in combination with others.\n",
    "- **Ignores Model Specificity:** Doesn't consider the specific machine learning algorithm being used.\n",
    "\n",
    "**Q5. When to Use Filter Methods**\n",
    "\n",
    "- **Large Datasets:** Efficient for handling a large number of features.\n",
    "- **Fast Feature Selection:** Useful for preliminary feature selection when computational budget is limited.\n",
    "- **Explanatory Feature Insights:** Easier to interpret why a feature is selected based on the statistical test.\n",
    "\n",
    "**Q6. Telecom Customer Churn - Filter Method**\n",
    "\n",
    "1. **Identify Target Variable:** Churn (customer leaves the service)\n",
    "2. **Feature Selection:**\n",
    "    - Calculate correlation coefficients between all features and the churn label.\n",
    "    - Choose features with high positive or negative correlations (indicating a strong relationship).\n",
    "    - Alternatively, use Chi-Square test for categorical features like customer service satisfaction.\n",
    "3. **Select Features:** Based on the calculated scores, choose features exceeding a set threshold or falling within a specific range of correlation.\n",
    "\n",
    "**Q7. Soccer Match Outcome - Embedded Method**\n",
    "\n",
    "1. **Target Variable:** Match outcome (win/loss/draw)\n",
    "2. **Embedded Model Choice:** Consider using a Random Forest model as it performs feature selection during training by splitting features with the highest predictive power for the outcome.\n",
    "3. **Model Training:** Train the Random Forest model on the dataset with all features.\n",
    "4. **Feature Importance:** Analyze the importance scores assigned by the model to each feature. This indicates which features (player statistics, team rankings) contributed most to the prediction.\n",
    "\n",
    "**Q8. House Price Prediction - Wrapper Method**\n",
    "\n",
    "1. **Target Variable:** House price\n",
    "2. **Wrapper Method Choice:** You could use a Recursive Feature Elimination (RFE) algorithm, which starts with all features and iteratively removes the least important feature based on a chosen model's performance (e.g., accuracy).\n",
    "3. **Model Selection:** Choose a model like Linear Regression that performs well for house price prediction.\n",
    "4. **Feature Elimination:** Train the model with all features. RFE iteratively removes the feature with the least impact on the model's performance (measured by a metric like R-squared). This process continues until a desired number of features or performance threshold is reached.\n",
    "\n",
    "**Choosing the Right Method:**\n",
    "\n",
    "The selection of feature selection methods depends on your dataset size, computational resources, and the desired level of interpretability. Filter methods are suitable for initial exploration and large datasets, while Wrapper and Embedded methods offer better feature selection tailored to your specific model but require more computational power."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

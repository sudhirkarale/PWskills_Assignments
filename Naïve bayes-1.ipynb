{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Q1. Bayes' Theorem**\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability that allows you to calculate the **posterior probability** of an event (hypothesis) occurring given some **evidence** (observed data). It essentially helps you update your belief about a hypothesis based on new information.\n",
    "\n",
    "**Q2. Formula for Bayes' Theorem**\n",
    "\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "```\n",
    "P(B | A) = (P(A | B) * P(B)) / P(A)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `P(B | A)`: The posterior probability of event B occurring given event A has already happened (what you want to calculate).\n",
    "- `P(A | B)`: The likelihood, or the probability of observing event A if event B is true.\n",
    "- `P(B)`: The prior probability of event B happening (initial belief about B before considering A).\n",
    "- `P(A)`: The total probability of observing event A (often used as a normalization factor).\n",
    "\n",
    "**Q3. Using Bayes' Theorem in Practice**\n",
    "\n",
    "Bayes' theorem has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Machine Learning:** Used in algorithms like Naive Bayes classification (explained in Q5) and spam filtering.\n",
    "- **Medical Diagnosis:** Updating the probability of a disease based on symptoms.\n",
    "- **Risk Assessment:** Estimating the probability of a financial crisis given economic indicators.\n",
    "- **Search Engines:** Ranking search results based on the relevance of web pages to a query.\n",
    "\n",
    "**Q4. Relationship between Bayes' Theorem and Conditional Probability**\n",
    "\n",
    "Conditional probability, denoted by `P(A | B)`, represents the probability of event A occurring given that event B has already happened. Bayes' theorem explicitly uses conditional probability to calculate the posterior probability, considering both the likelihood (`P(A | B)`) and the prior probability (`P(B)`) of events.\n",
    "\n",
    "**Q5. Choosing a Naive Bayes Classifier Type**\n",
    "\n",
    "There are several types of Naive Bayes classifiers, each with its own assumptions and strengths. Here's a guide to choosing the right type:\n",
    "\n",
    "- **Gaussian Naive Bayes:** Assumes continuous features follow Gaussian (normal) distributions. Suitable for numerical data.\n",
    "- **Multinomial Naive Bayes:** Assumes discrete features with a fixed number of possible values. Good for text classification where features are words or phrases.\n",
    "- **Bernoulli Naive Bayes:** Assumes binary features (0 or 1). Useful for scenarios with presence/absence or true/false values.\n",
    "\n",
    "The best choice depends on the characteristics of your data:\n",
    "\n",
    "- **Feature types (continuous vs. discrete vs. binary)**\n",
    "- **Number of possible values for each feature**\n",
    "\n",
    "**Q6. Naive Bayes Classification Assignment**\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "- Dataset with two features (X1, X2) and two classes (A, B)\n",
    "- New instance: X1 = 3, X2 = 4\n",
    "- Class frequency table provided.\n",
    "\n",
    "**Goal:** Use Naive Bayes to predict the class (A or B) for the new instance, assuming equal prior probabilities for each class.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "1. **Calculate Class Probabilities:**\n",
    "\n",
    "   For each class (A, B), we need to calculate the probability of the new instance (X1 = 3, X2 = 4) belonging to that class. Since we're using the Multinomial Naive Bayes assumption (discrete features), we'll calculate the product of probabilities for each feature value given the class.\n",
    "\n",
    "   **Class A:**\n",
    "   ```\n",
    "   P(A | X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A)\n",
    "   ```\n",
    "\n",
    "   Calculate these probabilities by looking up the corresponding frequencies in the table and dividing by the total number of instances in class A. Do the same for P(X2=4 | A).\n",
    "\n",
    "   **Class B:**\n",
    "   Similarly, calculate `P(B | X1=3, X2=4)` using the frequencies for class B.\n",
    "\n",
    "2. **Apply Bayes' Theorem (assuming equal prior probabilities):**\n",
    "\n",
    "   Since we're assuming equal prior probabilities for A and B, we don't need to consider the prior probabilities (P(A) and P(B)) in this case. The class with the higher posterior probability will be the predicted class.\n",
    "\n",
    "   **Predicted Class:**\n",
    "   - If `P(A | X1=3, X2=4) > P(B | X1=3, X2=4)`, the new instance is predicted to belong to class A.\n",
    "   - Otherwise, it's predicted to belong to class B.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

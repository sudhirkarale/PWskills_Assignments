{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. Mathematical Formula for a Linear SVM**\n",
    "\n",
    "For a linear SVM, the decision boundary is a hyperplane represented by the equation:\n",
    "\n",
    "```\n",
    "w^T * x + b = 0\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `w` is the weight vector, representing the direction of the hyperplane.\n",
    "- `x` is the input data point.\n",
    "- `b` is the bias term, which shifts the hyperplane.\n",
    "\n",
    "The goal of the SVM is to find the hyperplane that maximizes the margin between the data points of different classes. The margin is defined as the distance between the closest data points (support vectors) from each class to the hyperplane.\n",
    "\n",
    "**Q2. Objective Function of a Linear SVM**\n",
    "\n",
    "The objective function of a linear SVM minimizes a combination of two terms:\n",
    "\n",
    "1. **Margin Maximization:** Maximizes the margin between the hyperplane and the closest data points (support vectors).\n",
    "2. **Regularization:** Penalizes large weight vectors to prevent overfitting.\n",
    "\n",
    "The most common form of the objective function uses L2 regularization:\n",
    "\n",
    "```\n",
    "0.5 * ||w||^2 - 1/C * sum(hinge_loss(w^T * x_i + b, y_i))\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `||w||^2` is the L2 norm of the weight vector (measures its magnitude).\n",
    "- `C` is the regularization parameter (controls the trade-off between margin maximization and overfitting).\n",
    "- `hinge_loss` is the hinge loss function, which penalizes misclassified data points.\n",
    "- `y_i` is the true class label of data point `x_i`.\n",
    "\n",
    "**Q3. Kernel Trick in SVM**\n",
    "\n",
    "The kernel trick allows linear SVMs to handle non-linearly separable data. It maps the data points from the original input space to a higher-dimensional feature space where they become linearly separable. This is achieved using a kernel function that computes the inner product of data points in the feature space without explicitly calculating the mapping itself. Common kernel functions include:\n",
    "\n",
    "- **Linear Kernel:** `K(x, y) = x^T * y` (suitable for already linearly separable data)\n",
    "- **Polynomial Kernel:** `K(x, y) = (gamma * x^T * y + r)^d` (captures polynomial relationships)\n",
    "- **Gaussian Radial Basis Function (RBF Kernel):** `K(x, y) = exp(-gamma * ||x - y||^2)` (captures non-linear relationships)\n",
    "\n",
    "**Q4. Role of Support Vectors**\n",
    "\n",
    "Support vectors (SVs) are the data points that lie closest to the hyperplane on either side of the margin. They play a crucial role in defining the decision boundary and contribute the most to the model's generalization ability.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a binary classification problem with red and blue data points. The support vectors are the two closest data points (one red and one blue) to the hyperplane (decision boundary). These points define the margin and influence the orientation of the hyperplane. New data points are classified based on their position relative to the hyperplane defined by the support vectors.\n",
    "\n",
    "**Q5. Hyperplane, Margin, Soft Margin, and Hard Margin (Illustrative Examples and Graphs)**\n",
    "\n",
    "**Hyperplane:** A hyperplane is a decision boundary in a high-dimensional space that separates data points belonging to different classes. In the case of a 2D feature space, it's a line.\n",
    "\n",
    "**Margin:** The margin is the distance between the hyperplane and the closest data points (support vectors) from each class. A larger margin indicates a better separation between the classes and potentially improved generalization ability.\n",
    "\n",
    "**Hard Margin:** A hard margin SVM aims to find a hyperplane that perfectly separates the data points of different classes. However, this is not always achievable in real-world datasets.\n",
    "\n",
    "**Soft Margin:** A soft margin SVM allows for a certain degree of misclassification by introducing slack variables. These variables allow some data points to violate the margin, but they are penalized in the objective function. This helps the model handle non-perfectly separable data while still maintaining a good margin.\n",
    "\n",
    "**Illustrative Graphs:**\n",
    "\n",
    "[Image of Hyperplane, Margin, Soft Margin, and Hard Margin in SVM]\n",
    "\n",
    "**Q6. SVM Implementation with Iris Dataset**\n",
    "\n",
    "Here's a Python implementation using scikit-learn for a linear SVM classifier on the Iris dataset, along with exploring the impact of the regularization parameter `C`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## Train Linear SVM with different C values\n",
    "C_values = [0.01, 1, 100]  # Experiment with different values\n",
    "\n",
    "for C in C_values:\n",
    "    # Train the linear SVM classifier\n",
    "    svm_clf = LinearSVC(C=C)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels on testing set\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy (C={C}): {accuracy:.4f}\")\n",
    "\n",
    "    # Plot the decision boundary (using first two features for visualization)\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='plasma')\n",
    "    plt.title(f\"Linear SVM Decision Boundary (C={C})\")\n",
    "    plt.xlabel(\"Sepal length (cm)\")\n",
    "    plt.ylabel(\"Sepal width (cm)\")\n",
    "\n",
    "    # Get the separating hyperplane equation from the model\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # Plot the hyperplane\n",
    "    x_vals = np.linspace(X_test[:, 0].min(), X_test[:, 0].max(), 100)\n",
    "    y_vals = (-w[0] * x_vals - b) / w[1]\n",
    "    plt.plot(x_vals, y_vals, 'k-')\n",
    "\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **Import libraries:** Necessary libraries for data manipulation (`pandas`), scikit-learn functionalities (`load_iris`, `train_test_split`, `LinearSVC`, `accuracy_score`), and visualization (`matplotlib`).\n",
    "2. **Load Iris dataset:** Load the dataset using `load_iris()`.\n",
    "3. **Preprocessing:** Separate features (`X`) and target labels (`y`). Split the data into training and testing sets using `train_test_split`.\n",
    "4. **Train Linear SVM with different C values:**\n",
    "    - Define a list of `C` values to experiment with (regularization parameter).\n",
    "    - For each `C`:\n",
    "        - Create a `LinearSVC` object with the specified `C`.\n",
    "        - Train the model on the training set using `fit()`.\n",
    "        - Predict labels on the testing set using `predict()`.\n",
    "        - Calculate accuracy using `accuracy_score`.\n",
    "        - Print the accuracy for the current `C` value.\n",
    "        - Plot the decision boundary using the first two features for visualization.\n",
    "            - Use the model's coefficients (`coef_`) and intercept (`intercept_`) to obtain the hyperplane equation.\n",
    "            - Plot the hyperplane equation as a line on the scatter plot of the testing data.\n",
    "\n",
    "**Running the code:**\n",
    "\n",
    "This code will train linear SVM models with different `C` values and display the following:\n",
    "\n",
    "- Accuracy for each chosen `C` value.\n",
    "- Visualization of the decision boundary for each `C` value, highlighting how the margin and classifier behavior might change with different regularization strengths.\n",
    "\n",
    "**Bonus: Implementing SVM from Scratch (Conceptual Overview)**\n",
    "\n",
    "While implementing SVM from scratch can be a valuable learning experience, it's often computationally expensive and might not outperform optimized libraries like scikit-learn for most practical purposes. However, here's a conceptual overview of the steps involved:\n",
    "\n",
    "1. **Define the objective function:** Implement the objective function combining margin maximization and L2 regularization.\n",
    "2. **Gradient descent optimization:** Use a gradient descent algorithm to minimize the objective function, updating the weight vector (`w`) and bias term (`b`).\n",
    "3. **Kernel trick implementation (optional):** If using non-linear kernels, define a function to compute the kernel function (e.g., RBF kernel) for mapping data points to the"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier (Q1 & Q2)\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "A supervised learning algorithm that uses a tree-like structure to classify data. It works by recursively splitting the data based on features (attributes) to create a series of decision rules that lead to a predicted class label.\n",
    "\n",
    "**Mathematical Intuition (Simplified):**\n",
    "\n",
    "1. **Entropy:** Measures the randomness or uncertainty in a dataset. Lower entropy indicates a more homogenous dataset.\n",
    "2. **Information Gain:** Measures how much a specific feature reduces uncertainty when splitting the data. The feature with the highest information gain becomes the root node of the tree.\n",
    "3. **Recursive Splitting:** The process continues by recursively splitting the data at each node based on the remaining features that provide the highest information gain, ultimately creating a tree-like structure with decision rules at each branch.\n",
    "\n",
    "**Example (Binary Classification):**\n",
    "\n",
    "Imagine classifying emails as spam (positive class) or not spam (negative class). The decision tree might first split based on the presence of specific keywords (e.g., \"free money\"), then further refine based on sender address or other features.\n",
    "\n",
    "## Geometric Intuition (Q4)\n",
    "\n",
    "Decision trees can be visualized as a series of hyperplanes (decision boundaries) in a multidimensional space. Each split in the tree corresponds to a hyperplane that separates the data points belonging to different classes.\n",
    "\n",
    "**Making Predictions:**\n",
    "\n",
    "A new data point traverses the tree starting from the root node. Based on the feature values of the data point, it follows the branches corresponding to those values until reaching a leaf node. The class label associated with the leaf node becomes the predicted class for the data point.\n",
    "\n",
    "## Confusion Matrix (Q5)\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model on a set of data. It shows the number of data points for each possible combination of predicted and actual class labels.\n",
    "\n",
    "**Rows represent the actual class labels.**\n",
    "**Columns represent the predicted class labels.**\n",
    "\n",
    "**Values in each cell represent the number of data points.**\n",
    "\n",
    "**Confusion Matrix and Evaluation Metrics (Q6):**\n",
    "\n",
    "**Example:**\n",
    "\n",
    "| Predicted Class | Positive (Actual) | Negative (Actual) |\n",
    "|---|---|---|\n",
    "| Positive | True Positives (TP) | False Positives (FP) |\n",
    "| Negative | False Negatives (FN) | True Negatives (TN) |\n",
    "\n",
    "**Using the confusion matrix, we can calculate various metrics:**\n",
    "\n",
    "* **Accuracy:** (TP + TN) / Total Samples (overall proportion of correct predictions)\n",
    "* **Precision:** TP / (TP + FP) (measures the proportion of positive predictions that were actually correct)\n",
    "* **Recall:** TP / (TP + FN) (measures the proportion of actual positive cases that were correctly identified)\n",
    "* **F1-Score:** 2 * (Precision * Recall) / (Precision + Recall) (harmonic mean of precision and recall)\n",
    "\n",
    "## Choosing Evaluation Metrics (Q7)\n",
    "\n",
    "The most appropriate evaluation metric depends on the specific problem and its priorities. Here are some factors to consider:\n",
    "\n",
    "* **Data Balance:** If the data is imbalanced, metrics like F1-score or precision/recall might be more informative than just accuracy.\n",
    "* **Cost of Errors:** If certain types of errors (false positives or false negatives) are more costly, you might prioritize metrics relevant to that cost (e.g., precision for high-cost false positives).\n",
    "* **Overall Performance:** Depending on the application, you might choose a combination of metrics like accuracy, F1-score, and AUC for a more comprehensive evaluation.\n",
    "\n",
    "## Example: Precision vs. Recall (Q8 & Q9)\n",
    "\n",
    "**Precision (Most Important):**\n",
    "\n",
    "* **Scenario:** Classifying medical test results for a rare disease.\n",
    "* **Reasoning:** A false positive (identifying a healthy person as having the disease) can lead to unnecessary anxiety and procedures. High precision ensures most positive predictions are truly positive, minimizing unnecessary interventions.\n",
    "\n",
    "**Recall (Most Important):**\n",
    "\n",
    "* **Scenario:**  Identifying fraudulent transactions on a credit card.\n",
    "* **Reasoning:**  A false negative (missing a fraudulent transaction) can lead to financial losses. High recall ensures most actual fraudulent transactions are caught, minimizing financial risks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
